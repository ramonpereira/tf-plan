#!/usr/bin/env python3

# This file is part of tf-plan.

# tf-plan is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.

# tf-plan is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with tf-plan. If not, see <http://www.gnu.org/licenses/>.
#
# -----------------------------------------------------------------
#
# Author: Ramon Fraga Pereira
# This is a modified version of Thiago Bueno's tf-plan that allows visualization for the domains:
# - Car Parking;
# - 2D Navigation (with deceleration);
# - 2D Navigation (without deceleration);
# - 3D Navigation;
# - LQR Nav 1D;
# - LQR Nav 2D;
# - LQR Nav 2D Multi-units;
#
# -----------------------------------------------------------------

import matplotlib.pyplot as plt
import argparse
import numpy as np
import time
import os

import rddlgym
import tfrddlsim.viz

import tfplan
from tfplan.planners.environment import OnlinePlanning
from tfplan.planners.online import OnlineOpenLoopPlanner
from tfplan.planners.offline import OfflineOpenLoopPlanner
from tfplan.train.policy import OpenLoopPolicy
from tfplan.test.evaluator import ActionEvaluator
from tfplan.visualizer.car_parking_visualizer import CarParkingVisualizer
from tfplan.visualizer.lqr_visualizer import LQRVisualizer
from tfplan.visualizer.lqr_2d_visualizer import LQR2DVisualizer
from tfplan.visualizer.navigation_2d_visualizer import Navigation2DVisualizer
from tfplan.visualizer.navigation_3d_visualizer import Navigation3DVisualizer
from tfplan.visualizer.navigation_deceleration_visualizer import NavigationDecelerationVisualizer

def parse_args():
    new_visualizers = tuple(tfrddlsim.viz.visualizers) + ('car_parking', 'navigation_2d', 'navigation_3d', 'navigation_deceleration', 'lqr_1d_nav', 'lqr_2d_nav' , 'lqr_2d_multi_unit',)
    
    description = '''
    tf-plan (v{}): Planning via gradient-based optimization in TensorFlow.
    '''.format(tfplan.__version__)
    parser = argparse.ArgumentParser(description=description)
    parser.add_argument(
        'rddl',
        type=str,
        help='RDDL file or rddlgym domain id'
    )
    parser.add_argument(
        '-m', '--mode',
        default='offline',
        choices=['offline', 'online'],
        help='planning mode (default=offline)'
    )
    parser.add_argument(
        '-b', '--batch-size',
        type=int, default=128,
        help='number of trajectories in a batch (default=128)'
    )
    parser.add_argument(
        '-hr', '--horizon',
        type=int, default=40,
        help='number of timesteps (default=40)'
    )
    parser.add_argument(
        '-e', '--epochs',
        type=int, default=500,
        help='number of timesteps (default=500)'
    )
    parser.add_argument(
        '-lr', '--learning-rate',
        type=float, default=0.01,
        help='optimizer learning rate (default=0.001)'
    )
    parser.add_argument(
        '--viz',
        default='generic',
        choices=new_visualizers,
        help='type of visualizer (default=generic)'
    )
    parser.add_argument(
        '-v', '--verbose',
        action='store_true',
        help='verbosity mode'
    )
    return parser.parse_args()

def print_parameters(args):
    if args.verbose:
        print()
        print('Running tf-plan v{} ...'.format(tfplan.__version__))
        print('>> RDDL:            {}'.format(args.rddl))
        print('>> Planning mode:   {}'.format(args.mode))
        print('>> Horizon:         {}'.format(args.horizon))
        print('>> Batch size:      {}'.format(args.batch_size))
        print('>> Training epochs: {}'.format(args.epochs))
        print('>> Learning rate:   {}'.format(args.learning_rate))
        print()

def load_model(args):
    compiler = rddlgym.make(args.rddl, mode=rddlgym.SCG)
    compiler.batch_mode_on()
    return compiler

def optimize(compiler, args):
    planning = online_planning if args.mode == 'online' else offline_planning
    return planning(compiler, args.batch_size, args.horizon, args.epochs, args.learning_rate)

def offline_planning(compiler, batch_size, horizon, epochs, learning_rate):
    # optimize actions
    planner = OfflineOpenLoopPlanner(compiler, batch_size, horizon)
    planner.build(learning_rate)
    actions, policy_variables = planner.run(epochs)

    # evaluate solution
    plan = OpenLoopPolicy(compiler, 1, horizon)
    plan.build('test', initializers=policy_variables)
    evaluator = ActionEvaluator(compiler, plan)
    trajectories = evaluator.run()
    return trajectories

def online_planning(compiler, batch_size, horizon, epochs, learning_rate):
    # build online planner
    open_loop_planner = OnlineOpenLoopPlanner(compiler, batch_size, horizon, parallel_plans=False)
    open_loop_planner.build(learning_rate, epochs)

    # run plan-execute-monitor cycle and evaluate solution
    planner = OnlinePlanning(compiler, open_loop_planner)
    planner.build()
    
    #trajectories, total_time, avg_time, stddev_time = planner.run(horizon)
    trajectories, stats = planner.run(horizon)
    return trajectories

def print_performance(trajectories):
    print()
    print('>> total reward = {:.6f}'.format(np.sum(trajectories[-1])))
    print()

def display(compiler, trajectories, visualizer_type, verbose=True):
    if verbose:
        if visualizer_type == 'lqr_1d_nav':
            non_fluents, initial_state, states, actions, interms, rewards = trajectories
            states = [(s[0], s[1][0]) for s in states]
            
            t_states = states[0][1]
            v_states = states[1][1]
            x_states = states[2][1]

            step_states = list(range(0, len(x_states)))

            visualizer = LQRVisualizer()
            visualizer.render(step_states, t_states, v_states, x_states)

            data = np.column_stack((v_states, x_states))

            np.savetxt('LQR_1D_Navigation-trajectory.csv', data, delimiter=',')

        if visualizer_type == 'lqr_2d_nav':
            non_fluents, initial_state, states, actions, interms, rewards = trajectories
            states = [(s[0], s[1][0]) for s in states]
            
            t_states = states[0][1]
            vx_states = states[1][1]
            vy_states = states[2][1]
            x_states = states[3][1]
            y_states = states[4][1]

            '''
            print('#> t_states: ' + str(t_states))
            print('#> vx_states: ' + str(vx_states))
            print('#> vy_states: ' + str(vy_states))
            print('#> x_states: ' + str(x_states))
            print('#> y_states: ' + str(y_states))
            '''

            step_states = list(range(0, len(x_states)))

            visualizer = LQR2DVisualizer()
            visualizer.render(step_states, t_states, vx_states, vy_states, x_states, y_states)

            data = np.column_stack((vx_states, vy_states, x_states, y_states))
            np.savetxt('LQR_2D_Navigation-trajectory.csv', data, delimiter=',')            

        if visualizer_type == 'lqr_2d_multi_unit':
            non_fluents, initial_state, states, actions, interms, rewards = trajectories
            states = [(s[0], s[1][0]) for s in states]
            
            t_states = states[0][1]
            vx_states = states[1][1]
            vy_states = states[2][1]
            x_states = states[3][1]
            y_states = states[4][1]

            '''
            print(compiler.state_fluent_variables[1])
            print(compiler.state_fluent_variables[2])
            
            print(compiler.state_fluent_variables[3])
            print(compiler.state_fluent_variables[4])

            print('\n t:')
            print(t_states)
            '''

            vx_states_unit2 = [(vx[0]) for vx in vx_states]
            vx_states_unit1 = [(vx[1]) for vx in vx_states]

            vy_states_unit2 = [(vy[0]) for vy in vy_states]
            vy_states_unit1 = [(vy[1]) for vy in vy_states]

            '''
            print('\n vx:')
            print('v002: ' + str(vx_states_unit2))
            print('v001: ' + str(vx_states_unit1))
            print('\n vy:')
            print('v002: ' + str(vy_states_unit2))
            print('v001: ' + str(vy_states_unit1))
            '''

            x_states_unit2 = [(x[0]) for x in x_states]
            x_states_unit1 = [(x[1]) for x in x_states]

            y_states_unit2 = [(y[0]) for y in y_states]
            y_states_unit1 = [(y[1]) for y in y_states]

            '''
            print('\n x:')
            print('v002: ' + str(x_states_unit2))
            print('v001: ' + str(x_states_unit1))
            print('\n y:')
            print('v002: ' + str(y_states_unit2))
            print('v001: ' + str(y_states_unit1))
            '''

            step_states = list(range(0, len(x_states)))

            visualizer = LQGMultiUnit2DVisualizer()
            visualizer.render(step_states, t_states, vx_states_unit1, vx_states_unit2, vy_states_unit1, vy_states_unit2, x_states_unit1, x_states_unit2, y_states_unit1, y_states_unit2)
            
            data = np.column_stack((vx_states_unit2, vx_states_unit1, vy_states_unit2, vy_states_unit1, x_states_unit2, y_states_unit2, x_states_unit1, y_states_unit1))
            np.savetxt('LQR_2D_Navigation_Multi_Unit-trajectory.csv', data, delimiter=',')

        if visualizer_type == 'generic':
            visualizer = tfrddlsim.viz.visualizers.get(visualizer_type, 'generic')
            viz = visualizer(compiler, verbose)
            viz.render(trajectories)
            non_fluents, initial_state, states, actions, interms, rewards = trajectories

        if visualizer_type == 'navigation_deceleration':
            visualizer = NavigationDecelerationVisualizer(compiler, verbose)
            visualizer.render(trajectories)
            non_fluents, initial_state, states, actions, interms, rewards = trajectories

        if visualizer_type == 'navigation_2d':
            visualizer = Navigation2DVisualizer(compiler, verbose)
            visualizer.render(trajectories)
            non_fluents, initial_state, states, actions, interms, rewards = trajectories

        if visualizer_type == 'navigation_3d':
            visualizer = Navigation3DVisualizer(compiler, verbose)
            visualizer.render(trajectories)
            non_fluents, initial_state, states, actions, interms, rewards = trajectories            

if __name__ == '__main__':

    start_time = time.time()
    # parse CLI arguments
    args = parse_args()

    # print planner parameters
    print_parameters(args)

    #read and compile RDDL file
    compiler = load_model(args)

    # optimize actions
    trajectories = optimize(compiler, args)

    # report performance
    print_performance(trajectories)

    # render visualization    
    display(compiler, trajectories, args.viz, args.verbose)
    print("#> Total time: %s seconds" % (time.time() - start_time))