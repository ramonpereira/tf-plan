#!/usr/bin/env python3

# This file is part of tf-plan.

# tf-plan is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.

# tf-plan is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with tf-plan. If not, see <http://www.gnu.org/licenses/>.
#
# -----------------------------------------------------------------
#
# Author: Ramon Fraga Pereira
# This is a modified version of Thiago Bueno for generating a datasets to learn the dynamics of hybrid domains.
#
# -----------------------------------------------------------------

import sys
import os

import matplotlib.pyplot as plt
import argparse
import numpy as np

import rddlgym
import tfrddlsim.viz

import tfplan
from tfplan.planners.environment import OnlinePlanning
from tfplan.planners.online import OnlineOpenLoopPlanner
from tfplan.planners.offline import OfflineOpenLoopPlanner
from tfplan.train.policy import OpenLoopPolicy
from tfplan.test.evaluator import ActionEvaluator

def parse_args():
    new_visualizers = tuple(tfrddlsim.viz.visualizers)
    
    description = '''
    tf-plan (v{}): Planning via gradient-based optimization in TensorFlow.
    '''.format(tfplan.__version__)
    parser = argparse.ArgumentParser(description=description)
    parser.add_argument(
        'rddl',
        type=str,
        help='RDDL file or rddlgym domain id'
    )
    parser.add_argument(
        '-m', '--mode',
        default='offline',
        choices=['offline', 'online'],
        help='planning mode (default=offline)'
    )
    parser.add_argument(
        '-b', '--batch-size',
        type=int, default=128,
        help='number of trajectories in a batch (default=128)'
    )
    parser.add_argument(
        '-hr', '--horizon',
        type=int, default=40,
        help='number of timesteps (default=40)'
    )
    parser.add_argument(
        '-e', '--epochs',
        type=int, default=500,
        help='number of timesteps (default=500)'
    )
    parser.add_argument(
        '-lr', '--learning-rate',
        type=float, default=0.01,
        help='optimizer learning rate (default=0.001)'
    )
    parser.add_argument(
        '--viz',
        default='generic',
        choices=new_visualizers,
        help='type of visualizer (default=generic)'
    )
    parser.add_argument(
        '-v', '--verbose',
        action='store_true',
        help='verbosity mode'
    )
    return parser.parse_args()

def print_parameters(args):
    if args.verbose:
        print()
        print('Running tf-plan v{} ...'.format(tfplan.__version__))
        print('>> RDDL:            {}'.format(args.rddl))
        print('>> Planning mode:   {}'.format(args.mode))
        print('>> Horizon:         {}'.format(args.horizon))
        print('>> Batch size:      {}'.format(args.batch_size))
        print('>> Training epochs: {}'.format(args.epochs))
        print('>> Learning rate:   {}'.format(args.learning_rate))
        print()

def load_model(args):
    compiler = rddlgym.make(args.rddl, mode=rddlgym.SCG)
    compiler.batch_mode_on()
    return compiler

def optimize(compiler, args):
    planning = online_planning if args.mode == 'online' else offline_planning
    return planning(compiler, args.batch_size, args.horizon, args.epochs, args.learning_rate)

def offline_planning(compiler, batch_size, horizon, epochs, learning_rate):
    # optimize actions
    planner = OfflineOpenLoopPlanner(compiler, batch_size, horizon)
    planner.build(learning_rate)
    actions, policy_variables = planner.run(epochs)

    # evaluate solution
    plan = OpenLoopPolicy(compiler, 1, horizon)
    plan.build('test', initializers=policy_variables)
    evaluator = ActionEvaluator(compiler, plan)
    trajectories = evaluator.run()
    return trajectories

def online_planning(compiler, batch_size, horizon, epochs, learning_rate):
    # build online planner
    open_loop_planner = OnlineOpenLoopPlanner(compiler, batch_size, horizon, parallel_plans=False)
    open_loop_planner.build(learning_rate, epochs)

    # run plan-execute-monitor cycle and evaluate solution
    planner = OnlinePlanning(compiler, open_loop_planner)
    planner.build()
    trajectories, stats = planner.run(horizon)
    return trajectories

def print_performance(trajectories):
    print()
    print('>> total reward = {:.6f}'.format(np.sum(trajectories[-1])))
    print()

def display(compiler, trajectories, visualizer_type, verbose=True):
    if verbose:
        visualizer = tfrddlsim.viz.visualizers.get(visualizer_type, 'generic')
        viz = visualizer(compiler, verbose)
        viz.render(trajectories)

def generate_dataset(list_trajectories, directory_name):
    #
    # lqr_nav_1d and lqg_nav_1d
    # action: u, states: t, states: v, states: x
    # 
    header_data_file = 'action: u,states: t,states: v,states: x'
    header_label_file = 'states: t,states: v,states: x'

    data_content = []
    label_content = []
    data_content.append(header_data_file)
    label_content.append(header_label_file)    
    
    for trajectories in list_trajectories:
        non_fluents, initial_state, states, actions, interms, rewards = trajectories
        
        current_state = ''
        for s in initial_state:
            current_state = current_state + ',' + str(s[0])

        states = [(s[0], s[1][0]) for s in states]

        t_states = states[0][1]
        v_states = states[1][1]
        x_states = states[2][1]
        steps = list(range(0, args.horizon))        

        executed_actions = actions[0][1][0]
        for index in steps:
            action = executed_actions[index]

            action_current_state = str(action) + (',' if index > 0 else '') + current_state
            data_content.append(action_current_state)
            #print(action_current_state)

            next_state = str(t_states[index]) + ',' + str(v_states[index]) + ',' + str(x_states[index])
            current_state = next_state
            label_content.append(next_state)
            #print(next_state)
            #print()

    data_file = open(str(directory_name) + '-data.txt', 'w')
    for data in data_content:
        data_file.write(data + '\n')

    data_file.close()
    
    label_file = open(str(directory_name) + '-label.txt', 'w')
    for label in label_content:
        label_file.write(label + '\n')

    label_file.close()

def generate_dataset_2D_multi_unit(list_trajectories, directory_name):
    #
    # lqg_2d_nav_multi_unit
    # action: ux_v002, action: ux_v001, action: uy_v002, action: uy_v001, states: t, states: vx_v002, states: vx_v001, states: vy_v002, states: vy_v001, states: x_v002, states: x_v001, states: y_v002, states: y_v001
    # 
    header_data_file = 'action: ux_v002,action: ux_v001,action: uy_v002,action: uy_v001,states: t,states: vx_v002,states: vx_v001,states: vy_v002,states: vy_v001,states: x_v002,states: x_v001,states: y_v002,states: y_v001'
    header_label_file = 'states: t,states: vx_v002,states: vx_v001,states: vy_v002,states: vy_v001,states: x_v002,states: x_v001,states: y_v002,states: y_v001'

    data_content = []
    label_content = []
    data_content.append(header_data_file)
    label_content.append(header_label_file)    
    
    for trajectories in list_trajectories:
        # visualizer = tfrddlsim.viz.visualizers.get('generic', 'generic')
        # viz = visualizer(compiler, True)
        # viz.render(trajectories)
        
        non_fluents, initial_state, states, actions, interms, rewards = trajectories
        
        current_state = ''
        for i in initial_state:
            s = i.tolist()[0]
            if type(s) is float:
                current_state = current_state + ',' + str(s)
            else: current_state = current_state + ',' + str(s[0]) + ',' + str(s[1])

        #print('Initial state: ' + str(current_state))
        states = [(s[0], s[1][0]) for s in states]
                    
        t_states = states[0][1]
        vx_states = states[1][1]
        vy_states = states[2][1]
        x_states = states[3][1]
        y_states = states[4][1]
        
        steps = list(range(0, args.horizon))

        #print(actions)
        ux_actions = actions[0][1].tolist()
        uy_actions = actions[1][1].tolist()
        #print('0: ' + str(ux_actions))
        #print('1: ' + str(uy_actions))

        vx_states = vx_states.tolist()
        vy_states = vy_states.tolist()
        x_states = x_states.tolist()
        y_states = y_states.tolist()

        for index in steps:
            ux_action = ux_actions[0][index]
            uy_action = uy_actions[0][index]

            ux_v002 = ux_action[0]
            ux_v001 = ux_action[1]
            uy_v002 = uy_action[0]
            uy_v001 = uy_action[1]

            vx_state = vx_states[index]
            vy_state = vy_states[index]
            x_state = x_states[index]
            y_state = y_states[index]

            action_current_state = str(ux_v002) + ',' + str(ux_v001) + ',' + str(uy_v002) + ',' + str(uy_v001) + (',' if index > 0 else '') + current_state
            data_content.append(action_current_state)
            #print(action_current_state)

            next_state = str(t_states[index]) + ',' + str(vx_state[0]) + ',' + str(vx_state[1]) + ',' + str(vy_state[0]) + ',' + str(vy_state[1]) + ',' + str(x_state[0]) + ',' + str(x_state[1]) + ',' + str(y_state[0]) + ',' + str(y_state[1])
            current_state = next_state
            label_content.append(next_state)
            #print(next_state)
            #print()

    data_file = open(str(directory_name) + '-data.txt', 'w')
    for data in data_content:
        data_file.write(data + '\n')

    data_file.close()
    
    label_file = open(str(directory_name) + '-label.txt', 'w')
    for label in label_content:
        label_file.write(label + '\n')

    label_file.close()    

if __name__ == '__main__':

    # parse CLI arguments
    args = parse_args()

    # print planner parameters
    print_parameters(args)

    list_trajectories = []

    domain_name = args.rddl.split('/')[3]

    # read RDDL (problem instances) files in a directory and generate the trajectory for each instance in such directory.
    for filename in os.listdir(args.rddl):
        if ".rddl" in filename:
            # compile RDDL file
            print('###> Planning for RDDL file: ' + str(filename))
            compiler = rddlgym.make(args.rddl + filename, mode=rddlgym.SCG)
            compiler.batch_mode_on()

            # optimize actions
            trajectories = optimize(compiler, args)

            # report performance
            print_performance(trajectories)

            # render visualization    
            # display(compiler, trajectories, args.viz, args.verbose)
            list_trajectories.append(trajectories)

    if '2d' in domain_name:
        generate_dataset_2D_multi_unit(list_trajectories, domain_name)
    else: generate_dataset(list_trajectories, domain_name)
